{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "96520524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter_softmax, scatter_add\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torch_geometric.datasets import MNISTSuperpixels\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6298bbd9",
   "metadata": {},
   "source": [
    "#### transform superpixel graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa7bc26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeReorder(BaseTransform):\n",
    "    \"\"\"\n",
    "    Sorts the nodes based on their positions along the y = x line;\n",
    "    reorders based on projections onto the [1, 1] vector.\n",
    "    Add positional embeddings and adjacency matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, unit_vec=None):\n",
    "        if unit_vec is None:\n",
    "            self.unit_vec = torch.tensor([1., 1.]) / (2 ** 0.5)  # univec along y = x\n",
    "        else:\n",
    "            self.unit_vec = torch.tensor(unit_vec)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # Compute projections onto the unit vector\n",
    "        projections = data.pos @ self.unit_vec\n",
    "        # Get new order by sorting the projections\n",
    "        new_order = projections.argsort()\n",
    "        # Create mapping\n",
    "        mapping = torch.zeros_like(new_order)\n",
    "        mapping[new_order] = torch.arange(len(new_order), dtype=torch.long)\n",
    "\n",
    "        # Reorder node features\n",
    "        if hasattr(data, 'laplacian_eigen_pe'):\n",
    "            data.x = torch.cat([data.x, data.laplacian_eigen_pe], dim=1)\n",
    "            del data.laplacian_eigen_pe\n",
    "\n",
    "        data.x = data.x[new_order]\n",
    "        data.pos = data.pos[new_order]\n",
    "\n",
    "        # Update edge indices\n",
    "        data.edge_index = mapping[data.edge_index]\n",
    "        \n",
    "        # Compute adjacency matrix A directly using PyTorch tensors\n",
    "        num_nodes = data.num_nodes\n",
    "        edge_index = data.edge_index\n",
    "        A = torch.zeros((num_nodes, num_nodes), dtype=torch.float32)\n",
    "        A[edge_index[0], edge_index[1]] = 1.0\n",
    "        data.adj = A  # Add adjacency matrix to data\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "984e7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# laplacian positional embed + node reorder along y = x\n",
    "transform = T.Compose([\n",
    "    T.AddLaplacianEigenvectorPE(k=5, attr_name='laplacian_eigen_pe'),\n",
    "    NodeReorder()\n",
    "])\n",
    "\n",
    "mnist_path = '/Users/subat/Desktop/datan/hierarchical gAVE/model dev/mnist_superpixel'\n",
    "train_transformed = MNISTSuperpixels(root=mnist_path, transform=transform)\n",
    "test_transformed = MNISTSuperpixels(root=mnist_path, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8294fb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(train_transformed[0].adj, cmap=\"crest\", cbar=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbcb0ea",
   "metadata": {},
   "source": [
    "## hierarchical gvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2061bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # element-wise projection of z_A instead of z_A[batch] (z_A_i instead of W_A_i)\n",
    "\n",
    "#         # s_i = W_h h_i + b_h + W_A z_A + b_A\n",
    "#         self.W_h = nn.Linear(gcn_hidden_dim, zi_dim)          # W_h: maps h_i to s_i\n",
    "#         self.W_A = nn.Linear(za_dim, zi_dim, baise=False)   # W_A: maps z_A to s_i\n",
    "        \n",
    "#         if self.node_specific_loading:\n",
    "#             # Node-specific vectors for projecting z_A from [za_dim] to [num_nodes, za_dim]\n",
    "#             self.W_Ap = nn.Parameter(torch.randn(num_nodes, za_dim))\n",
    "            \n",
    "# # forward\n",
    "#         # ===== 4. Z_i; q(Z_i|Z_A,X,A) =====\n",
    "#         if self.node_specific_loading:\n",
    "            \n",
    "#             # List to hold the projected data\n",
    "#             z_A_projected = []\n",
    "#             # For each graph in the batch\n",
    "#             for i in range(batch_size):\n",
    "#                 # Project each node through node-specific vectors in W_Ap\n",
    "#                 transformed = z_A[i].unsqueeze(0) * W_Ap  # element-wise multiplication - node-specific projection of z_A\n",
    "#                 z_A_projected.append(transformed)\n",
    "#             z_A_projected = torch.cat(z_A_projected, dim=0)    # [num_nodes, za_dim]\n",
    "#             # Linear combination: s_i = W_h h_i + W_A z_A_projected \n",
    "#             s_i = self.W_h(h) + self.W_A(z_A_projected)        # [num_nodes, zi_dim]\n",
    "            \n",
    "#         else: \n",
    "#             # Expand z_A to match the number of nodes\n",
    "#             z_A_expanded = z_A[batch]                          # [num_nodes, za_dim]\n",
    "#             # Linear combination: s_i = W_h h_i + W_A z_A \n",
    "#             s_i = self.W_h(h) + self.W_A(z_A_expanded)         # [num_nodes, zi_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c59606ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class inference_model(nn.Module):\n",
    "    def __init__(self, hparams: dict, node_specific_loading: bool = True):\n",
    "        super(inference_model, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.node_specific_loading = node_specific_loading\n",
    "\n",
    "        self.num_nodes_per_graph = hparams['num_nodes_per_graph']   # N        \n",
    "        self.gcn_hidden_dim = hparams['gcn_hidden_dim']             # Nh\n",
    "        self.za_dim = hparams['za_dim']                             # G\n",
    "        self.zi_dim = hparams['zi_dim']                             # F\n",
    "\n",
    "        # Graph convolutional layers (3-hop)\n",
    "        self.conv1 = GCNConv(hparams['node_feature_dim'], self.gcn_hidden_dim)\n",
    "        self.conv2 = GCNConv(self.gcn_hidden_dim, self.gcn_hidden_dim)\n",
    "        self.conv3 = GCNConv(self.gcn_hidden_dim, self.gcn_hidden_dim)\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(self.gcn_hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(self.gcn_hidden_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(self.gcn_hidden_dim)\n",
    "        \n",
    "        # Attention mechanism to compute attention scores per node\n",
    "        # φ = φ_1 * h1 + φ_2 * h2 + φ_bias                   [num_nodes,1]\n",
    "        # attn_weights = softmax(φ)                          [num_nodes,1]\n",
    "        # φ_weighted = atten_weights * h3                    [num_nodes, gcn_hidden_dim]\n",
    "        # h_graph = sum(φ_weighted)                          [batch_size, gcn_hidden_dim]\n",
    "        self.phi_h1 = nn.Linear(self.gcn_hidden_dim, 1)\n",
    "        self.phi_h2 = nn.Linear(self.gcn_hidden_dim, 1, bias=False)\n",
    "\n",
    "        # Linear layers to compute μ_A and log Σ_A\n",
    "        self.fc_mu_A = nn.Linear(self.gcn_hidden_dim, self.za_dim)\n",
    "        self.fc_logvar_A = nn.Linear(self.gcn_hidden_dim, self.za_dim)\n",
    "        \n",
    "        # Linear layers for node-level latent variables s_i (linear combination of h and z_a)\n",
    "        # s_i = W_h h_i + b_h + W_A z_A + b_A\n",
    "        self.W_h = nn.Linear(self.gcn_hidden_dim, self.zi_dim)           # W_h: maps h_i to s_i\n",
    "        \n",
    "        if self.node_specific_loading:         \n",
    "            # Node-specific weight matrices for transforming z_A when computing s_i\n",
    "            # s_i = W_h h_i + b_h + W_A_i z_A + b_A\n",
    "            self.W_A = nn.Parameter(torch.randn(self.num_nodes_per_graph, self.za_dim, self.zi_dim))\n",
    "        else:\n",
    "            self.W_A = nn.Linear(self.za_dim, self.zi_dim, bias=False)   # W_A: maps z_A to s_i\n",
    "        \n",
    "        # Linear layers to compute μ_i and log Σ_i\n",
    "        self.fc_mu_i = nn.Linear(self.zi_dim, self.zi_dim)\n",
    "        self.fc_logvar_i = nn.Linear(self.zi_dim, self.zi_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node feature matrix of shape [num_nodes, node_feature_dim].\n",
    "            edge_index (Tensor): Edge indices.\n",
    "\n",
    "        Returns:\n",
    "            z_i (Tensor): Sampled node-level latent variables.\n",
    "            mu_i (Tensor): Mean of the node-level latent variables.\n",
    "            logvar_i (Tensor): Log variance of the node-level latent variables.\n",
    "            z_A (Tensor): Sampled graph-level latent variable.\n",
    "            mu_A (Tensor): Mean of the graph-level latent variable.\n",
    "            logvar_A (Tensor): Log variance of the graph-level latent variable.\n",
    "        \"\"\"      \n",
    "        # ===== 1. Node hidden states with GCN =====\n",
    "        h1 = torch.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        h2 = torch.relu(self.bn2(self.conv2(h1, edge_index)))\n",
    "        h3 = torch.relu(self.bn3(self.conv3(h2, edge_index)))  # [num_nodes, gcn_hidden_dim] \n",
    "          \n",
    "        # ===== 2. Attention Pooling to aggregate node-level hidden state to graph-level =====\n",
    "        # Options: 1) global pooling; 2) hierarchical (local) pooling\n",
    "        # Review hierarchical pooling, e.g., Self-Attention Graph Pooling (Lee et al., 19')\n",
    "        # Global pooling implemented below\n",
    "        \n",
    "        # Compute attention scores for each node at h1 and h2\n",
    "        phi = self.phi_h1(h1) + self.phi_h2(h2)                # [num_nodes, 1]\n",
    "        attn_scores = phi.squeeze(-1)                          # [num_nodes]\n",
    "\n",
    "        # Compute attention weights using softmax over nodes in the same graph\n",
    "        attn_weights = scatter_softmax(attn_scores, batch)     # [num_nodes]\n",
    "        # Weight node embeddings\n",
    "        attn_weights = attn_weights.unsqueeze(-1)              # [num_nodes, 1]\n",
    "        phi_weighted = h3 * attn_weights                       # [num_nodes, gcn_hidden_dim]\n",
    "        # Sum over nodes per graph to get graph-level representation\n",
    "        h_graph = scatter_add(phi_weighted, batch, dim=0)      # [batch_size, gcn_hidden_dim]\n",
    "        \n",
    "        # h_graph = global_mean_pool(h, batch) # simple global mean pooling\n",
    "\n",
    "        # ===== 3. Z_A; q(Z_A∣X,A) =====\n",
    "        # Compute μ_A and log Σ_A\n",
    "        mu_A = self.fc_mu_A(h_graph)            # [batch_size, za_dim] - Mean μ_h\n",
    "        logvar_A = self.fc_logvar_A(h_graph)    # [batch_size, za_dim] - Log variance log Σ_h\n",
    "        \n",
    "        # Reparameterization trick for Z_A\n",
    "        std_A = torch.exp(0.5 * logvar_A)\n",
    "        eps_A = torch.randn_like(std_A)\n",
    "        z_A = mu_A + eps_A * std_A              # [batch_size, za_dim]\n",
    "        \n",
    "        # ===== 4. Z_i; q(Z_i|Z_A,X,A) =====\n",
    "        batch_size = batch.max().item() + 1\n",
    "        z_A_expanded = z_A[batch]               # [num_nodes, za_dim]\n",
    "        \n",
    "        if self.node_specific_loading:\n",
    "            \n",
    "            # Reshape z_A to [batch_size, num_nodes_per_graph, za_dim] for batch matrix multiplication\n",
    "            z_A_reshaped = z_A_expanded.view(batch_size, self.num_nodes_per_graph, self.za_dim).transpose(0, 1)\n",
    "            # Perform batch matrix multiplication\n",
    "            # W_A: [num_nodes_per_graph, za_dim, zi_dim]; ngf\n",
    "            # z_A_reshaped: [num_nodes_per_graph, batch_size, za_dim]; nbg\n",
    "            # z_A_transformed: [num_nodes_per_graph, batch_size, zi_dim]; nbf\n",
    "            z_A_transformed = torch.einsum('ngf,nbg->nbf', self.W_A, z_A_reshaped)\n",
    "            # Reshape to [num_nodes_per_graph * batch_size, zi_dim]\n",
    "            z_A_transformed = z_A_transformed.transpose(0, 1).reshape(self.num_nodes_per_graph * batch_size, self.zi_dim)\n",
    "            \n",
    "            # Linear combination: s_i = W_h h_i + W_A_i z_A\n",
    "            s_i = self.W_h(h3) + z_A_transformed               # [num_nodes, zi_dim]\n",
    "            \n",
    "        else: \n",
    "            # Linear combination: s_i = W_h h_i + W_A z_A \n",
    "            s_i = self.W_h(h3) + self.W_A(z_A_expanded)        # [num_nodes, zi_dim]\n",
    "        \n",
    "        # Compute μ_i and log Σ_i\n",
    "        mu_i = self.fc_mu_i(s_i)                # [num_nodes, zi_dim]\n",
    "        logvar_i = self.fc_logvar_i(s_i)        # [num_nodes, zi_dim]\n",
    "        \n",
    "        # Reparameterization trick for Z_i\n",
    "        std_i = torch.exp(0.5 * logvar_i)\n",
    "        eps_i = torch.randn_like(std_i)\n",
    "        z_i = mu_i + eps_i * std_i              # [num_nodes, zi_dim]\n",
    "        \n",
    "        return z_i, mu_i, logvar_i, z_A, mu_A, logvar_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c07f1ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class generative_model(nn.Module):\n",
    "    def __init__(self, hparams: dict):\n",
    "        super(generative_model, self).__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        zi_dim = hparams['zi_dim']                      # F\n",
    "        node_feature_dim = hparams['node_feature_dim']  # D\n",
    "        attention_dim = hparams['attention_dim']        # Dimension for Q, K, V \n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        # Attention layers\n",
    "        self.query_linear = nn.Linear(zi_dim, attention_dim)\n",
    "        self.key_linear = nn.Linear(zi_dim, attention_dim)\n",
    "        self.value_linear = nn.Linear(zi_dim, attention_dim)\n",
    "\n",
    "        # Node feature decoder (reconstructs X from z_i and A_hat)\n",
    "        # GCN and MLP?\n",
    "        # self.node_decoder = \n",
    "        \n",
    "    def adj_decoder(self, z_i, batch):\n",
    "        \"\"\"\n",
    "        Reconstruct adjacency matrices for each graph.\n",
    "\n",
    "        Args:\n",
    "            z_i (Tensor): Node-level latent variables [num_nodes, zi_dim].\n",
    "            batch (Tensor): Batch vector assigning each node to a graph in the batch.\n",
    "\n",
    "        Returns:\n",
    "            A_hat_batch (Tensor): Reconstructed adjacency matrices [batch, num_nodes, num_nodes].\n",
    "        \"\"\"\n",
    "        num_graphs = batch.max().item() + 1\n",
    "        A_hat_list = []\n",
    "\n",
    "        for i in range(num_graphs):\n",
    "            mask = (batch == i)\n",
    "            Z = z_i[mask]                   # [num_nodes_in_graph_i, zi_dim]\n",
    "\n",
    "            # Compute adjacency matrix reconstruction for each graph\n",
    "            \n",
    "            # ===== Attention decoder =====\n",
    "            # Compute Query, Key, Value\n",
    "            Q = self.query_linear(Z)        # [num_nodes_in_graph_i, attention_dim]\n",
    "            K = self.key_linear(Z)          # [num_nodes_in_graph_i, attention_dim]\n",
    "            V = self.value_linear(Z)        # [num_nodes_in_graph_i, attention_dim]\n",
    "            \n",
    "            attention_weights = Q @ K.T                        # [num_nodes_in_graph_i, num_nodes_in_graph_i]\n",
    "            attention_weights = attention_weights.softmax(1)   # same shape, but attention_weights.sum(1) = ones\n",
    "            weighted_values = (attention_weights.unsqueeze(-1) * V.unsqueeze(0)).sum(1)   # [num_nodes_in_graph_i, attention_dim]\n",
    "            Z_att = weighted_values / (self.attention_dim**0.5)\n",
    "        \n",
    "            A_hat = torch.sigmoid(torch.matmul(Z_att, Z_att.t()))  # [num_nodes_in_graph_i, num_nodes_in_graph_i]\n",
    "            \n",
    "            # Diag(A_hat) is ~ 1, while Diag(A) is 0\n",
    "            # ***Options - 1) set diag(A_hat) to 0; 2) set diag(A) to 1 for BCE\n",
    "            A_hat.fill_diagonal_(0) # Option 1\n",
    "            A_hat_list.append(A_hat)\n",
    "\n",
    "        return torch.stack(A_hat_list, dim=0)        \n",
    "\n",
    "    def forward(self, z_i, batch):\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder.\n",
    "\n",
    "        Args:\n",
    "            z_i (Tensor): Node-level latent variables [num_nodes, zi_dim].\n",
    "            batch (Tensor): Batch vector assigning each node to a graph in the batch.\n",
    "\n",
    "        Returns:\n",
    "            X_hat (Tensor): Reconstructed node features.\n",
    "        \"\"\"\n",
    "        device = z_i.device\n",
    "        num_nodes = z_i.size(0)\n",
    "        num_graphs = batch.max().item() + 1\n",
    "        \n",
    "        # Reconstruct A\n",
    "        A_hat_batch = self.adj_decoder(z_i, batch)\n",
    "\n",
    "        # Initialize reconstructed node features\n",
    "        X_hat = torch.zeros((num_nodes, self.hparams['node_feature_dim']), device=device)\n",
    "        \n",
    "#         # Process each graph individually\n",
    "#         for i in range(num_graphs):\n",
    "#             # Nodes belonging to graph i\n",
    "#             mask = (batch == i)\n",
    "#             node_indices = torch.where(mask)[0]\n",
    "#             num_nodes_in_graph = mask.sum().item()\n",
    "\n",
    "#             # Extract z_i for graph i\n",
    "#             z_i_graph = z_i[mask]  # [num_nodes_in_graph, zi_dim]\n",
    "\n",
    "#             # Get reconstructed adjacency matrix A_hat for graph i\n",
    "#             A_hat = A_hat_batch[i]  # [num_nodes_in_graph, num_nodes_in_graph]\n",
    "\n",
    "#             # Convert dense matrix to edge_index and edge_weight\n",
    "#             edge_index, edge_weight = dense_to_sparse(A_hat)\n",
    "\n",
    "#             # GCN and MLP layers ?\n",
    "#             X_hat_graph = self.node_decoder(z_i_graph, edge_index, edge_weight)\n",
    "#             X_hat[mask] = X_hat_graph\n",
    "        \n",
    "        return A_hat_batch, X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "73a2db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphVAELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes the loss function for the HierarchicalGVAE.\n",
    "\n",
    "    method args:\n",
    "        A_hat_batch (Tensor): Reconstructed adjacency matrices [batch, num_nodes, num_nodes].\n",
    "        # X_hat (Tensor): Reconstructed node features [num_nodes, node_feature_dim].\n",
    "        data (Batch): Original data batch containing edge_index, x, and batch attributes.\n",
    "        mu_A (Tensor): Mean of z_A [batch_size, za_dim].\n",
    "        logvar_A (Tensor): Log variance of of z_A [batch_size, za_dim].\n",
    "        mu_i (Tensor): Mean of z_i [num_nodes, zi_dim].\n",
    "        logvar_i (Tensor): Log variance of z_i [num_nodes, zi_dim].\n",
    "        device (torch.device): CPU/GPU.\n",
    "\n",
    "    method returns:\n",
    "        total_loss (Tensor): Total loss for the batch.\n",
    "        adj_recon_term (Tensor): Reconstruction loss for adjacency matrices.\n",
    "        # node_feature_recon_term (Tensor): Reconstruction loss for node features.\n",
    "        kl_za_term (Tensor): KL divergence loss for graph-level latent variables.\n",
    "        kl_zi_term (Tensor): KL divergence loss for node-level latent variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 device: torch.device, \n",
    "                 sparsity_weight: float = 1.0, \n",
    "                 beta_A: float = 1.0, \n",
    "                 beta_i: float = 1.0\n",
    "                ):\n",
    "        assert 0.0 < sparsity_weight <= 1.0\n",
    "        self.sparsity_weight = sparsity_weight\n",
    "        self.beta_A = beta_A\n",
    "        self.beta_i = beta_i\n",
    "        self.device = device\n",
    "        super(GraphVAELoss, self).__init__()\n",
    "        \n",
    "    @staticmethod\n",
    "    def kl_between_two_gaussians(m1, logvar1, m2, logvar2):\n",
    "        \"\"\"\n",
    "        KL[ N(m1, logvar1) || N(m2, logvar2) ]\n",
    "            i.e. both distributions parameterised by log of diagonal of covariance matrix\n",
    "                 \n",
    "        Shapes:\n",
    "            m1: [batch, dim]\n",
    "            logvar1: [batch, dim]\n",
    "            m2: [batch, dim]\n",
    "            logvar2: [batch, dim]\n",
    "            \n",
    "        Output: [batch], all elements > 0.0\n",
    "        \"\"\"\n",
    "        det1 = logvar1.sum(-1).exp()   # [batch]\n",
    "        det2 = logvar2.sum(-1).exp()   # [batch]\n",
    "        det_term = det1 / det2         # [batch]\n",
    "        \n",
    "        d_term = m1.shape[1]\n",
    "        \n",
    "        trace_term = (logvar1 - logvar2).exp().sum(-1)   # [batch]\n",
    "        \n",
    "        mean_diff = m2 - m1\n",
    "        inner_prod_term = (mean_diff.square() / logvar2.exp()).sum(-1) # [batch]\n",
    "        \n",
    "        return 0.5 * det_term - d_term + trace_term + inner_prod_term    \n",
    "    \n",
    "    def kl_zi_term(self, mu_i, logvar_i, z_a, all_B_mus, all_B_logvars):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            mu_i: [batch, num_nodes, dim_zi]\n",
    "            logvar_i: [batch, num_nodes, dim_zi]\n",
    "            z_a: [batch, dim_za]\n",
    "            all_B_mus: [num_nodes, dim_zi, dim_za]\n",
    "            all_B_logvars: [num_nodes, dim_zi, dim_za]\n",
    "        \n",
    "        Sum of each KL to node-wise prior:\n",
    "            Sum_i KL[ N(mu_i, logvar) || N(...) ]\n",
    "        \"\"\"\n",
    "        z_a = z_a.detach()\n",
    "        N = all_B_logvars.shape[0]\n",
    "        \n",
    "        all_prior_means = torch.einsum('nfg,bg->bnf', all_B_mus, z_a).detach()         # [batch, num_nodes, dim_zi]\n",
    "        all_prior_logvars = torch.einsum('nfg,bg->bnf', all_B_logvars, z_a).detach()   # [batch, num_nodes, dim_zi]\n",
    "        \n",
    "        cumulative_kls = 0.0  # [batch]\n",
    "        \n",
    "        for node_index in range(N):\n",
    "            prior_mean = all_prior_means[:,node_index]      # [batch, dim_zi]\n",
    "            prior_logvar = all_prior_logvars[:,node_index]  # [batch, dim_zi]\n",
    "            node_kl = self.kl_between_two_gaussians(\n",
    "                mu_i[:,node_index], logvar_i[:,node_index],\n",
    "                prior_mean, prior_logvar)    # [batch]\n",
    "            cumulative_kls = cumulative_kls + node_kl\n",
    "            \n",
    "        return cumulative_kls.mean()   # scalar\n",
    "\n",
    "    def kl_za_term(self, mu_A, logvar_A):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            mu_A: [batch, dim_za]\n",
    "            logvar_A: [batch, dim_za]\n",
    "        \n",
    "        KL to prior:\n",
    "            KL[ N(mu, logvar) || N(O,I) ]\n",
    "            scalar size\n",
    "        \"\"\"\n",
    "        prior_mean = torch.zeros_like(mu_A, device = mu_A.device, dtype = mu_A.dtype)\n",
    "        prior_logvar = torch.zeros_like(mu_A, device = mu_A.device, dtype = mu_A.dtype)\n",
    "        return self.kl_between_two_gaussians(mu_A, logvar_A, prior_mean, prior_logvar).mean()\n",
    "        \n",
    "    def adj_recon_term(self, A_hat_batch, A_true):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            A_hat_batch of shape [batch, N, N]\n",
    "                A_hat_batch[b,i,j] = p(A_b[i,j] = 1 | ...)\n",
    "                \n",
    "            A_true of shape [batch, N, N]\n",
    "                A_true[b,i,j] = A_b[i,j] \\in {0, 1}\n",
    "                \n",
    "        Cross entropy (elementwise):\n",
    "            log( p(A)^A * (1-p(A))^(1-A) ) = Alogp(A) + (1-A)logp(1-A)\n",
    "        \"\"\"\n",
    "        A_hat_batch[A_hat_batch == 0.0] = 1e-10\n",
    "        A_hat_batch[A_hat_batch == 1.0] = 1. - 1e-10\n",
    "        \n",
    "        log_likelihood = (\n",
    "            (A_true * A_hat_batch.log()) + \n",
    "            ((1.0 - A_true) * (1.0 - A_hat_batch).log())\n",
    "        )\n",
    "        \n",
    "        log_likelihood[A_true == 0.0] = log_likelihood[A_true == 0.0] * self.sparsity_weight\n",
    "            \n",
    "        return log_likelihood.sum(-1).sum(-1).mean()\n",
    "        \n",
    "    def node_feature_recon_term(self, X_hat, X_true):\n",
    "        return 0.0\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        A_hat_batch, A_true, X_hat, X_true, \n",
    "        mu_A, logvar_A, mu_i, logvar_i, z_a, \n",
    "        all_B_mus, all_B_logvars\n",
    "    ):\n",
    "\n",
    "        A_recon = self.adj_recon_term(A_hat_batch, A_true)\n",
    "        X_recon = self.node_feature_recon_term(X_hat, X_true)\n",
    "        KL_A = self.kl_za_term(mu_A, logvar_A)\n",
    "        KL_i = self.kl_zi_term(mu_i, logvar_i, z_a, all_B_mus, all_B_logvars)\n",
    "        \n",
    "        return {\n",
    "            'A_recon_loss': A_recon,\n",
    "            'X_recon': X_recon,\n",
    "            'KL_A': KL_A,\n",
    "            'KL_i': KL_i,\n",
    "            'negative_beta_ELBO': - (A_recon + X_recon + self.beta_A * KL_A + self.beta_i * KL_i)\n",
    "        }\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b986d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d35aa94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c44cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0de09269",
   "metadata": {},
   "source": [
    "## eval trail-n-error "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3fcfe9",
   "metadata": {},
   "source": [
    "##### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "132aad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_transformed, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_transformed, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8f68d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference model\n",
    "hparams = {\n",
    "    'num_nodes_per_graph': 75,\n",
    "    'node_feature_dim': train_transformed.num_features,  # Number of node features\n",
    "    'gcn_hidden_dim': 64,  \n",
    "    'za_dim': 8,          # Dimension of graph-level latent variable z_A\n",
    "    'zi_dim': 4,          # Dimension of node-level latent variable z_i\n",
    "    'attention_dim': 16\n",
    "}\n",
    "encoder = inference_model(hparams, node_specific_loading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "44e47430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one batch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder.to(device)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "batch = batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fe82afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass through encoder\n",
    "num_graphs = batch.num_graphs\n",
    "num_nodes_per_graph = batch.num_nodes // batch.num_graphs\n",
    "adj_batch = torch.stack([data.adj for data in batch.to_data_list()], dim=0)  # [num_graphs, node_dim, node_dim]\n",
    "\n",
    "z_i, mu_i, logvar_i, z_A, mu_A, logvar_A = encoder(batch.x, batch.edge_index, batch.batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "523c7ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2400, 4]) torch.Size([32, 8])\n"
     ]
    }
   ],
   "source": [
    "print(z_i.shape, z_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6bed4cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Global attention\n",
    "\n",
    "# h.shape\n",
    "# global_mean_pool(h, batch.batch).shape\n",
    "\n",
    "# attention_net = nn.Linear(64, 1)\n",
    "\n",
    "# attn_scores = attention_net(h).squeeze(-1)\n",
    "# attn_weights = scatter_softmax(attn_scores, batch.batch)\n",
    "# attn_weights = attn_weights.unsqueeze(-1)              # [num_nodes, 1]\n",
    "# h_weighted = h * attn_weights                          # [num_nodes, gcn_hidden_dim]\n",
    "# # h_graph = scatter_add(h_weighted, batch.batch, dim=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a992c7b",
   "metadata": {},
   "source": [
    "###### generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "17b15e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generative model\n",
    "decoder = generative_model(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "39b12099",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_hat_batch = decoder(z_i, batch.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "afec3a0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(A_hat_batch[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrest\u001b[39m\u001b[38;5;124m\"\u001b[39m, cbar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "sns.heatmap(A_hat_batch[2].detach().cpu().numpy(), cmap=\"crest\", cbar=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "35a8a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_hat recon\n",
    "# z_trial = z_i[:75]\n",
    "# adj_trial = A_hat_batch[0].clone().fill_diagonal_(0)\n",
    "# conv_trial = GCNConv(8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "056c850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.utils import dense_to_sparse\n",
    "# edge_index, edge_weight = dense_to_sparse(adj_trial)\n",
    "# x_n = conv_trial.forward(z_trial, edge_index, edge_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a663bb4b",
   "metadata": {},
   "source": [
    "#### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b6886c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes_per_graph = 75\n",
    "\n",
    "hparams = {\n",
    "    'num_nodes_per_graph': num_nodes_per_graph,\n",
    "    'node_feature_dim': train_transformed.num_features,  # Number of node features\n",
    "    'gcn_hidden_dim': 64,  \n",
    "    'za_dim': 8,          # Dimension of graph-level latent variable z_A\n",
    "    'zi_dim': 4,          # Dimension of node-level latent variable z_i\n",
    "    'attention_dim': 16\n",
    "}\n",
    "encoder = inference_model(hparams, node_specific_loading=True)\n",
    "decoder = generative_model(hparams)\n",
    "\n",
    "# define loss function\n",
    "loss_func = GraphVAELoss(device = 'cpu')\n",
    "\n",
    "# SGD\n",
    "parameters = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = optim.SGD(parameters, lr=0.05, weight_decay=0.01)  # weight decay for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "200cd35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dependece of priors over zi conditioned on za\n",
    "            # all_B_mus: [num_nodes, dim_zi, dim_za]\n",
    "            # all_B_logvars: [num_nodes, dim_zi, dim_za]\n",
    "\n",
    "B_mus_linear_condtional_dependence = torch.randn(\n",
    "    hparams['num_nodes_per_graph'], hparams['zi_dim'], hparams['za_dim'], \n",
    ")\n",
    "B_logvars_linear_condtional_dependence = torch.randn(\n",
    "    hparams['num_nodes_per_graph'], hparams['zi_dim'], hparams['za_dim'], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5cded78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(encoder, decoder, optimizer, loss_fn, train_loader, all_B_mus, all_B_logvars):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    epoch_tracking = {\n",
    "        \"A_recon_loss\": [],\n",
    "        \"X_recon\": [],\n",
    "        \"KL_A\": [],\n",
    "        \"KL_i\": [],\n",
    "        \"negative_beta_ELBO\": [],\n",
    "    }\n",
    "        \n",
    "    for batch in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        num_graphs = batch.num_graphs\n",
    "        num_nodes_per_graph = batch.num_nodes // batch.num_graphs\n",
    "        A_batch = torch.stack([data.adj for data in batch.to_data_list()], dim=0)  # [num_graphs, node_dim, node_dim]\n",
    "        X_batch = batch.x # [num_nodes, x_dim]\n",
    "        \n",
    "        # inference model\n",
    "        z_i, mu_i, logvar_i, z_A, mu_A, logvar_A = encoder(batch.x, batch.edge_index, batch.batch)\n",
    "        # generative model\n",
    "        A_hat_batch, X_hat = decoder(z_i, batch.batch)\n",
    "        \n",
    "        losses_dict = loss_fn(\n",
    "            A_hat_batch, A_batch, X_hat, X_batch, \n",
    "            mu_A, logvar_A, mu_i, logvar_i, z_A, \n",
    "            all_B_mus, all_B_logvars\n",
    "        )\n",
    "\n",
    "        losses_dict['negative_beta_ELBO'].backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        for k in losses_dict.keys():\n",
    "            epoch_tracking[k].append(losses_dict[k].item())\n",
    "    \n",
    "    return epoch_tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "208770e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m tracking \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA_recon_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_recon\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative_beta_ELBO\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 12\u001b[0m     epoch_tracking \u001b[38;5;241m=\u001b[39m train_epoch(\n\u001b[1;32m     13\u001b[0m         encoder, decoder, \n\u001b[1;32m     14\u001b[0m         optimizer, loss_func, \n\u001b[1;32m     15\u001b[0m         train_loader, \n\u001b[1;32m     16\u001b[0m         B_mus_linear_condtional_dependence,\n\u001b[1;32m     17\u001b[0m         B_logvars_linear_condtional_dependence\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     20\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m epoch_tracking\u001b[38;5;241m.\u001b[39mkeys():\n",
      "Cell \u001b[0;32mIn[120], line 26\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(encoder, decoder, optimizer, loss_fn, train_loader, all_B_mus, all_B_logvars)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# generative model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m A_hat_batch, X_hat \u001b[38;5;241m=\u001b[39m decoder(z_i, batch\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m---> 26\u001b[0m losses_dict \u001b[38;5;241m=\u001b[39m loss_fn(\n\u001b[1;32m     27\u001b[0m     A_hat_batch, A_batch, X_hat, X_batch, \n\u001b[1;32m     28\u001b[0m     mu_A, logvar_A, mu_i, logvar_i, z_A, \n\u001b[1;32m     29\u001b[0m     all_B_mus, all_B_logvars\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m losses_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative_beta_ELBO\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[106], line 144\u001b[0m, in \u001b[0;36mGraphVAELoss.forward\u001b[0;34m(self, A_hat_batch, A_true, X_hat, X_true, mu_A, logvar_A, mu_i, logvar_i, z_a, all_B_mus, all_B_logvars)\u001b[0m\n\u001b[1;32m    142\u001b[0m X_recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_feature_recon_term(X_hat, X_true)\n\u001b[1;32m    143\u001b[0m KL_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkl_za_term(mu_A, logvar_A)\n\u001b[0;32m--> 144\u001b[0m KL_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkl_zi_term(mu_i, logvar_i, z_a, all_B_mus, all_B_logvars)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA_recon_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: A_recon,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_recon\u001b[39m\u001b[38;5;124m'\u001b[39m: X_recon,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative_beta_ELBO\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m (A_recon \u001b[38;5;241m+\u001b[39m X_recon \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_A \u001b[38;5;241m*\u001b[39m KL_A \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_i \u001b[38;5;241m*\u001b[39m KL_i)\n\u001b[1;32m    152\u001b[0m }\n",
      "Cell \u001b[0;32mIn[106], line 86\u001b[0m, in \u001b[0;36mGraphVAELoss.kl_zi_term\u001b[0;34m(self, mu_i, logvar_i, z_a, all_B_mus, all_B_logvars)\u001b[0m\n\u001b[1;32m     84\u001b[0m     prior_mean \u001b[38;5;241m=\u001b[39m all_prior_means[:,node_index]      \u001b[38;5;66;03m# [batch, dim_zi]\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     prior_logvar \u001b[38;5;241m=\u001b[39m all_prior_logvars[:,node_index]  \u001b[38;5;66;03m# [batch, dim_zi]\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     node_kl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkl_between_two_gaussians(\n\u001b[1;32m     87\u001b[0m         mu_i[:,node_index], logvar_i[:,node_index],\n\u001b[1;32m     88\u001b[0m         prior_mean, prior_logvar)    \u001b[38;5;66;03m# [batch]\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     cumulative_kls \u001b[38;5;241m=\u001b[39m cumulative_kls \u001b[38;5;241m+\u001b[39m node_kl\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cumulative_kls\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[106], line 54\u001b[0m, in \u001b[0;36mGraphVAELoss.kl_between_two_gaussians\u001b[0;34m(m1, logvar1, m2, logvar2)\u001b[0m\n\u001b[1;32m     51\u001b[0m det2 \u001b[38;5;241m=\u001b[39m logvar2\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexp()   \u001b[38;5;66;03m# [batch]\u001b[39;00m\n\u001b[1;32m     52\u001b[0m det_term \u001b[38;5;241m=\u001b[39m det1 \u001b[38;5;241m/\u001b[39m det2         \u001b[38;5;66;03m# [batch]\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m d_term \u001b[38;5;241m=\u001b[39m m1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     56\u001b[0m trace_term \u001b[38;5;241m=\u001b[39m (logvar1 \u001b[38;5;241m-\u001b[39m logvar2)\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# [batch]\u001b[39;00m\n\u001b[1;32m     58\u001b[0m mean_diff \u001b[38;5;241m=\u001b[39m m2 \u001b[38;5;241m-\u001b[39m m1\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "tracking = {\n",
    "    \"A_recon_loss\": [],\n",
    "    \"X_recon\": [],\n",
    "    \"KL_A\": [],\n",
    "    \"KL_i\": [],\n",
    "    \"negative_beta_ELBO\": [],\n",
    "}\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    \n",
    "    epoch_tracking = train_epoch(\n",
    "        encoder, decoder, \n",
    "        optimizer, loss_func, \n",
    "        train_loader, \n",
    "        B_mus_linear_condtional_dependence,\n",
    "        B_logvars_linear_condtional_dependence\n",
    "    )\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    for k in epoch_tracking.keys():\n",
    "        tracking[k].extend(epoch_tracking[k])\n",
    "    \n",
    "        plt.plot(tracking[k], label = k)\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9fd66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
